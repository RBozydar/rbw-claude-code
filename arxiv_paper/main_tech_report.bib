@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{belcak2025small,
  title={Small Language Models are the Future of Agentic AI},
  author={Belcak, Peter and Heinrich, Greg and Diao, Shizhe and Fu, Yonggan and Dong, Xin and Muralidharan, Saurav and Lin, Yingyan Celine and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2506.02153},
  year={2025}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Yang and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{phan2025humanity,
  title={Humanity's last exam},
  author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and others},
  journal={arXiv preprint arXiv:2501.14249},
  year={2025}
}

@article{barres2025tau,
  title = "{${\tau}^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment}",
  author={Barres, Victor and Dong, Honghua and Ray, Soham and Si, Xujie and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2506.07982},
  year={2025}
}

@misc{krishna2024factfetchreasonunified,
      title={Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation}, 
      author={Satyapriya Krishna and Kalpesh Krishna and Anhad Mohananey and Steven Schwarcz and Adam Stambler and Shyam Upadhyay and Manaal Faruqui},
      year={2024},
      eprint={2409.12941},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12941}, 
}

@misc{gpt-5,
  title = {Introducing GPT-5},
  howpublished = {\url{https://openai.com/index/introducing-gpt-5/}},
  note = {Accessed: 2025-09-23},
  author={OpenAI},
}

@misc{gpt-4o,
  title = {Hello GPT-4o},
  howpublished = {\url{https://openai.com/index/hello-gpt-4o/}},
  note = {Accessed: 2025-09-23},
  author={OpenAI},
}

@misc{o3-mini,
  title = {OpenAI o3-mini},
  howpublished = {\url{https://openai.com/index/openai-o3-mini/}},
  note = {Accessed: 2025-09-23},
  author={OpenAI},
}

@article{nathani2025mlgym,
  title={Mlgym: A new framework and benchmark for advancing ai research agents},
  author={Nathani, Deepak and Madaan, Lovish and Roberts, Nicholas and Bashlykov, Nikolay and Menon, Ajay and Moens, Vincent and Budhiraja, Amar and Magka, Despoina and Vorotilov, Vladislav and Chaurasia, Gaurav and others},
  journal={arXiv preprint arXiv:2502.14499},
  year={2025}
}

@article{qin2024tool,
  title={Tool learning with foundation models},
  author={Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Zhou, Xuanhe and Huang, Yufei and Xiao, Chaojun and others},
  journal={ACM Computing Surveys},
  volume={57},
  number={4},
  pages={1--40},
  year={2024},
  publisher={ACM New York, NY}
}

@article{abouelenin2025phi,
  title={Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras},
  author={Abouelenin, Abdelrahman and Ashfaq, Atabak and Atkinson, Adam and Awadalla, Hany and Bach, Nguyen and Bao, Jianmin and Benhaim, Alon and Cai, Martin and Chaudhary, Vishrav and Chen, Congcong and others},
  journal={arXiv preprint arXiv:2503.01743},
  year={2025}
}

@article{bercovich2025llama,
  title={Llama-nemotron: Efficient reasoning models},
  author={Bercovich, Akhiad and Levy, Itay and Golan, Izik and Dabbah, Mohammad and El-Yaniv, Ran and Puny, Omri and Galil, Ido and Moshe, Zach and Ronen, Tomer and Nabwani, Najeeb and others},
  journal={arXiv preprint arXiv:2505.00949},
  year={2025}
}

@article{team2025gemma,
  title={Gemma 3 technical report},
  author={{Google}, Team and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}

@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{gehring2024rlef,
  title={Rlef: Grounding code llms in execution feedback with reinforcement learning},
  author={Gehring, Jonas and Zheng, Kunhao and Copet, Jade and Mella, Vegard and Carbonneaux, Quentin and Cohen, Taco and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2410.02089},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{zhang2025qwen3,
  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},
  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2506.05176},
  year={2025}
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

@article{xi2024agentgym,
  title={Agentgym: Evolving large language model-based agents across diverse environments},
  author={Xi, Zhiheng and Ding, Yiwen and Chen, Wenxiang and Hong, Boyang and Guo, Honglin and Wang, Junzhe and Yang, Dingwen and Liao, Chenyang and Guo, Xin and He, Wei and others},
  journal={arXiv preprint arXiv:2406.04151},
  year={2024}
}

@article{zhou2024archer,
  title={Archer: Training language model agents via hierarchical multi-turn rl},
  author={Zhou, Yifei and Zanette, Andrea and Pan, Jiayi and Levine, Sergey and Kumar, Aviral},
  journal={arXiv preprint arXiv:2402.19446},
  year={2024}
}

@article{xi2025agentgym,
  title={AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning},
  author={Xi, Zhiheng and Huang, Jixuan and Liao, Chenyang and Huang, Baodai and Guo, Honglin and Liu, Jiaqi and Zheng, Rui and Ye, Junjie and Zhang, Jiazheng and Chen, Wenxiang and others},
  journal={arXiv preprint arXiv:2509.08755},
  year={2025}
}

@inproceedings{yao2023react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{press2023measuring,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={5687--5711},
  year={2023}
}

@article{shen2023hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={38154--38180},
  year={2023}
}

@inproceedings{wu2024autogen,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents}
}

@inproceedings{li2023camel,
  title={CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society},
  author={Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68539--68551},
  year={2023}
}

@inproceedings{qintoolllm,
  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{qian2024tell,
  title={Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents},
  author={Qian, Cheng and He, Bingxiang and Zhuang, Zhong and Deng, Jia and Qin, Yujia and Cong, Xin and Zhang, Zhong and Zhou, Jie and Lin, Yankai and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1088--1113},
  year={2024}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{yu2024steptool,
  title={StepTool: Enhancing Multi-Step Tool Usage in LLMs through Step-Grained Reinforcement Learning},
  author={Yu, Yuanqing and Wang, Zhefan and Ma, Weizhi and Wang, Shuai and Wu, Chuhan and Guo, Zhiqiang and Zhang, Min},
  journal={arXiv preprint arXiv:2410.07745},
  year={2024}
}

@article{goldie2025synthetic,
  title={Synthetic data generation \& multi-step rl for reasoning \& tool use},
  author={Goldie, Anna and Mirhoseini, Azalia and Zhou, Hao and Cai, Irene and Manning, Christopher D},
  journal={arXiv preprint arXiv:2504.04736},
  year={2025}
}

@inproceedings{asai2023self,
  title={Self-rag: Self-reflective retrieval augmented generation},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  booktitle={NeurIPS 2023 workshop on instruction tuning and instruction following},
  year={2023}
}

@article{yu2024auto,
  title={Auto-rag: Autonomous retrieval-augmented generation for large language models},
  author={Yu, Tian and Zhang, Shaolei and Feng, Yang},
  journal={arXiv preprint arXiv:2411.19443},
  year={2024}
}

@article{jin2025search,
  title={Search-r1: Training llms to reason and leverage search engines with reinforcement learning},
  author={Jin, Bowen and Zeng, Hansi and Yue, Zhenrui and Yoon, Jinsung and Arik, Sercan and Wang, Dong and Zamani, Hamed and Han, Jiawei},
  journal={arXiv preprint arXiv:2503.09516},
  year={2025}
}

@article{song2025r1,
  title={R1-searcher: Incentivizing the search capability in llms via reinforcement learning},
  author={Song, Huatong and Jiang, Jinhao and Min, Yingqian and Chen, Jie and Chen, Zhipeng and Zhao, Wayne Xin and Fang, Lei and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2503.05592},
  year={2025}
}

@article{chen2025learning,
  title={Learning to reason with search for llms via reinforcement learning},
  author={Chen, Mingyang and Li, Tianpeng and Sun, Haoze and Zhou, Yijie and Zhu, Chenzheng and Wang, Haofen and Pan, Jeff Z and Zhang, Wen and Chen, Huajun and Yang, Fan and others},
  journal={arXiv preprint arXiv:2503.19470},
  year={2025}
}

@article{zheng2025deepresearcher,
  title={Deepresearcher: Scaling deep research via reinforcement learning in real-world environments},
  author={Zheng, Yuxiang and Fu, Dayuan and Hu, Xiangkun and Cai, Xiaojie and Ye, Lyumanshan and Lu, Pengrui and Liu, Pengfei},
  journal={arXiv preprint arXiv:2504.03160},
  year={2025}
}

@article{tan2025rag,
  title={RAG-R1: Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism},
  author={Tan, Zhiwen and Huang, Jiaming and Wu, Qintong and Zhang, Hongxuan and Zhuang, Chenyi and Gu, Jinjie},
  journal={arXiv preprint arXiv:2507.02962},
  year={2025}
}

@article{li2025webthinker,
  title={Webthinker: Empowering large reasoning models with deep research capability},
  author={Li, Xiaoxi and Jin, Jiajie and Dong, Guanting and Qian, Hongjin and Zhu, Yutao and Wu, Yongkang and Wen, Ji-Rong and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2504.21776},
  year={2025}
}

@article{sun2025simpledeepsearcher,
  title={SimpleDeepSearcher: Deep information seeking via web-powered reasoning trajectory synthesis},
  author={Sun, Shuang and Song, Huatong and Wang, Yuhao and Ren, Ruiyang and Jiang, Jinhao and Zhang, Junjie and Bai, Fei and Deng, Jia and Zhao, Wayne Xin and Liu, Zheng and others},
  journal={arXiv preprint arXiv:2505.16834},
  year={2025}
}

@article{li2025websailor,
  title={WebSailor: Navigating Super-human Reasoning for Web Agent},
  author={Li, Kuan and Zhang, Zhongwang and Yin, Huifeng and Zhang, Liwen and Ou, Litu and Wu, Jialong and Yin, Wenbiao and Li, Baixuan and Tao, Zhengwei and Wang, Xinyu and others},
  journal={arXiv preprint arXiv:2507.02592},
  year={2025}
}

@article{tao2025webshaper,
  title={Webshaper: Agentically data synthesizing via information-seeking formalization},
  author={Tao, Zhengwei and Wu, Jialong and Yin, Wenbiao and Zhang, Junkai and Li, Baixuan and Shen, Haiyang and Li, Kuan and Zhang, Liwen and Wang, Xinyu and Jiang, Yong and others},
  journal={arXiv preprint arXiv:2507.15061},
  year={2025}
}

@article{zhang2025nemotron,
  title={Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning},
  author={Zhang, Shaokun and Dong, Yi and Zhang, Jieyu and Kautz, Jan and Catanzaro, Bryan and Tao, Andrew and Wu, Qingyun and Yu, Zhiding and Liu, Guilin},
  journal={arXiv preprint arXiv:2505.00024},
  year={2025}
}

@article{qian2025toolrl,
  title={Toolrl: Reward is all tool learning needs},
  author={Qian, Cheng and Acikgoz, Emre Can and He, Qi and Wang, Hongru and Chen, Xiusi and Hakkani-T{\"u}r, Dilek and Tur, Gokhan and Ji, Heng},
  journal={arXiv preprint arXiv:2504.13958},
  year={2025}
}

@article{qiu2025alita,
  title={Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution},
  author={Qiu, Jiahao and Qi, Xuan and Zhang, Tongcheng and Juan, Xinzhe and Guo, Jiacheng and Lu, Yifu and Wang, Yimin and Yao, Zixin and Ren, Qihan and Jiang, Xun and others},
  journal={arXiv preprint arXiv:2505.20286},
  year={2025}
}

@article{fang2025cognitive,
  title={Cognitive kernel-pro: A framework for deep research agents and agent foundation models training},
  author={Fang, Tianqing and Zhang, Zhisong and Wang, Xiaoyang and Wang, Rui and Qin, Can and Wan, Yuxuan and Ma, Jun-Yu and Zhang, Ce and Chen, Jiaqi and Li, Xiyun and others},
  journal={arXiv preprint arXiv:2508.00414},
  year={2025}
}

@article{li2025chain,
  title={Chain-of-agents: End-to-end agent foundation models via multi-agent distillation and agentic rl},
  author={Li, Weizhen and Lin, Jianbo and Jiang, Zhuosong and Cao, Jingyi and Liu, Xinpeng and Zhang, Jiayu and Huang, Zhenqiang and Chen, Qianben and Sun, Weichen and Wang, Qiexiang and others},
  journal={arXiv preprint arXiv:2508.13167},
  year={2025}
}

@report{monicaim_manus_2025,
  author       = {{Monica.im}},
  title        = {Manus AI},
  type         = {Technical report},
  institution  = {{Monica.im}},
  year         = {2025},
  url          = {https://manus.im/}
}

@online{moonshot_kimi_k2_2025,
  author   = {{Moonshot AI}},
  title    = {Kimi-K2},
  year     = {2025},
  date     = {2025-07-11},
  url      = {https://github.com/MoonshotAI/Kimi-K2},
  urldate  = {2025-07-25}
}

@online{moonshot_kimi_researcher_2025,
  author   = {{Moonshot AI}},
  title    = {Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities},
  year     = {2025},
  date     = {2025-06-20},
  url      = {https://moonshotai.github.io},
  urldate  = {2025-07-25}
}

@report{openai_deep_research_2025,
  author       = {{OpenAI}},
  title        = {Introducing Deep Research},
  type         = {Technical report},
  institution  = {{OpenAI}},
  year         = {2025},
  url          = {https://openai.com/index/introducing-deep-research/}
}

@online{perplexity_deep_research_2025,
  author   = {{Perplexity AI}},
  title    = {Introducing Perplexity Deep Research},
  year     = {2025},
  date     = {2025-02-14},
  url      = {https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research},
  urldate  = {2025-07-25}
}

@online{deepmind_gemini_deep_research_2025,
  author  = {{Google DeepMind}},
  title   = {Gemini Deep Research — Your Personal Research Assistant},
  year    = {2025},
  url     = {https://gemini.google.com},
  urldate = {2025-07-25}
}

@Misc{smolagents,
  title =        {`smolagents`: a smol library to build great agentic systems.},
  author =       {Aymeric Roucher and Albert Villanova del Moral and Thomas Wolf and Leandro von Werra and Erik Kaunismäki},
  howpublished = {\url{https://github.com/huggingface/smolagents}},
  year =         {2025}
}

@article{wu2025webdancer,
  title={WebDancer: Towards Autonomous Information Seeking Agency},
  author={Wu, Jialong and Li, Baixuan and Fang, Runnan and Yin, Wenbiao and Zhang, Liwen and Tao, Zhengwei and Zhang, Dingchu and Xi, Zekun and Fu, Gang and Jiang, Yong and others},
  journal={arXiv preprint arXiv:2505.22648},
  year={2025}
}


@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}

@article{toshniwal2024openmath,
  title   = {OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset},
  author  = {Shubham Toshniwal and Ivan Moshkov and Sean Narenthiran and Daria Gitman and Fei Jia and Igor Gitman},
  year    = {2024},
  journal = {arXiv preprint arXiv: Arxiv-2402.10176}
}

@misc{generalthought,
  author={Jarius, otaldohenrikk, knight raider, supahao, alpam, Esac, gonros, tomsercu, ryan, sidoneytemporary977, panpan, Tim, arpitg1991, Doge, tginart, pcpthm, eli5, yych, caijie, yuchen.zhang2003, lockon, susheelt, wangxinjing, duyiyang, Slimane, FABGYUXIN, chendarcy, Sin, robintan, imhillxtz, navinahc, z, zhangdapao, yixiangRDS500},
  title={GeneralThought-430K},
  year={2025},
  howpublished={\url{https://huggingface.co/datasets/RJT1990/GeneralThoughtArchive}}
}

@misc{codestral,
  author={Mistral AI team},
  title={Codestral},
  year={2024},
  howpublished={\url{https://mistral.ai/news/codestral}}
}

@misc{claude41,
  author={{Anthropic}},
  title={Claude Opus 4.1},
  year={2025},
  howpublished={\url{https://www.anthropic.com/news/claude-opus-4-1}}
}


@article{hu2025owl,
  title={Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation},
  author={Hu, Mengkang and Zhou, Yuhang and Fan, Wendong and Nie, Yuzhou and Xia, Bowei and Sun, Tao and Ye, Ziyu and Jin, Zhaoxuan and Li, Yingru and Chen, Qiguang and others},
  journal={arXiv preprint arXiv:2505.23885},
  year={2025}
}

@article{bahdanau2024tapeagents,
  title={Tapeagents: a holistic framework for agent development and optimization},
  author={Bahdanau, Dzmitry and Gontier, Nicolas and Huang, Gabriel and Kamalloo, Ehsan and Pardinas, Rafael and Pich{\'e}, Alex and Scholak, Torsten and Shliazhko, Oleh and Tremblay, Jordan Prince and Ghanem, Karam and others},
  journal={arXiv preprint arXiv:2412.08445},
  year={2024}
}

@article{wei2025browsecomp,
  title={Browsecomp: A simple yet challenging benchmark for browsing agents},
  author={Wei, Jason and Sun, Zhiqing and Papay, Spencer and McKinney, Scott and Han, Jeffrey and Fulford, Isa and Chung, Hyung Won and Passos, Alex Tachard and Fedus, William and Glaese, Amelia},
  journal={arXiv preprint arXiv:2504.12516},
  year={2025}
}

@article{wu2025webwalker,
  title={Webwalker: Benchmarking llms in web traversal},
  author={Wu, Jialong and Yin, Wenbiao and Jiang, Yong and Wang, Zhenglin and Xi, Zekun and Fang, Runnan and Zhang, Linhai and He, Yulan and Zhou, Deyu and Xie, Pengjun and others},
  journal={arXiv preprint arXiv:2501.07572},
  year={2025}
}

@inproceedings{mialon2023gaia,
  title={Gaia: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{tang2025autoagent,
  title={AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents},
  author={Tang, Jiabin and Fan, Tianyu and Huang, Chao},
  journal={arXiv preprint arXiv:2502.05957},
  year={2025}
}

@article{zhu2025oagents,
  title={Oagents: An empirical study of building effective agents},
  author={Zhu, He and Qin, Tianrui and Zhu, King and Huang, Heyuan and Guan, Yeyi and Xia, Jinxiang and Yao, Yi and Li, Hanhao and Wang, Ningning and Liu, Pai and others},
  journal={arXiv preprint arXiv:2506.15741},
  year={2025}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@inproceedings{wang2025self,
  title={Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions},
  author={Wang, Hongru and Xue, Boyang and Zhou, Baohang and Zhang, Tianhua and Wang, Cunxiang and Wang, Huimin and Chen, Guanhua and Wong, Kam-Fai},
  booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6510--6525},
  year={2025}
}

@article{qian2025smart,
  title={Smart: Self-aware agent for tool overuse mitigation},
  author={Qian, Cheng and Acikgoz, Emre Can and Wang, Hongru and Chen, Xiusi and Sil, Avirup and Hakkani-T{\"u}r, Dilek and Tur, Gokhan and Ji, Heng},
  journal={arXiv preprint arXiv:2502.11435},
  year={2025}
}

@article{aggarwal2025l1,
  title={L1: Controlling how long a reasoning model thinks with reinforcement learning},
  author={Aggarwal, Pranjal and Welleck, Sean},
  journal={arXiv preprint arXiv:2503.04697},
  year={2025}
}

@article{arora2025training,
  title={Training language models to reason efficiently},
  author={Arora, Daman and Zanette, Andrea},
  journal={arXiv preprint arXiv:2502.04463},
  year={2025}
}

@article{wang2025harnessing,
  title={Harnessing the reasoning economy: A survey of efficient reasoning for large language models},
  author={Wang, Rui and Wang, Hongru and Xue, Boyang and Pang, Jianhui and Liu, Shudong and Chen, Yi and Qiu, Jiahao and Wong, Derek Fai and Ji, Heng and Wong, Kam-Fai},
  journal={arXiv preprint arXiv:2503.24377},
  year={2025}
}

@article{wang2025otc,
  title={Otc: Optimal tool calls via reinforcement learning},
  author={Wang, Hongru and Qian, Cheng and Zhong, Wanjun and Chen, Xiusi and Qiu, Jiahao and Huang, Shijue and Jin, Bowen and Wang, Mengdi and Wong, Kam-Fai and Ji, Heng},
  journal={arXiv e-prints},
  pages={arXiv--2504},
  year={2025}
}

@article{zhao2025llm,
  title={LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios},
  author={Zhao, Bingxi and Foo, Lin Geng and Hu, Ping and Theobalt, Christian and Rahmani, Hossein and Liu, Jun},
  journal={arXiv preprint arXiv:2508.17692},
  year={2025}
}

@article{wang2025efficient,
  title={Efficient Agents: Building Effective Agents While Reducing Cost},
  author={Wang, Ningning and Hu, Xavier and Liu, Pai and Zhu, He and Hou, Yue and Huang, Heyuan and Zhang, Shengyu and Yang, Jian and Liu, Jiaheng and Zhang, Ge and others},
  journal={arXiv preprint arXiv:2508.02694},
  year={2025}
}

@article{peng2025agentic,
  title={Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems},
  author={Peng, Hao and Qi, Yunjia and Wang, Xiaozhi and Yao, Zijun and Xu, Bin and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2502.19328},
  year={2025}
}

@article{li2025torl,
  title={Torl: Scaling tool-integrated rl},
  author={Li, Xuefeng and Zou, Haoyang and Liu, Pengfei},
  journal={arXiv preprint arXiv:2503.23383},
  year={2025}
}

@misc{compound-ai-blog,
  title={The Shift from Models to Compound AI Systems},
  author={Matei Zaharia and Omar Khattab and Lingjiao Chen and Jared Quincy Davis
          and Heather Miller and Chris Potts and James Zou and Michael Carbin
          and Jonathan Frankle and Naveen Rao and Ali Ghodsi},
  howpublished={\url{https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/}},
  year={2024}
}

@inproceedings{chaudhry2025towards,
  title={Towards Resource-Efficient Compound AI Systems},
  author={Chaudhry, Gohar Irfan and Choukse, Esha and Goiri, {\'I}{\~n}igo and Fonseca, Rodrigo and Belay, Adam and Bianchini, Ricardo},
  booktitle={Proceedings of the 2025 Workshop on Hot Topics in Operating Systems},
  pages={218--224},
  year={2025}
}

@inproceedings{burns2024weak,
  title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  booktitle={International Conference on Machine Learning},
  pages={4971--5012},
  year={2024},
  organization={PMLR}
}

